{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to Github repository: https://github.com/fridamariejoergensen/CSS_assignment2.git\n",
    "\n",
    "### Contribution Statement\n",
    "\n",
    "We, Frida(s206182), Cecilie(s214605), and Marie(s204052), collaborated on Assignment 2 in Jupyter Notebook.\n",
    "\n",
    "All members collaborated and contributed to multiple parts of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fridajorgensen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_community\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Mixing Patterns and Assortativity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Mixing Patterns and Assortativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Communities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Zachary's karate club:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Community detection on the network of Computational Social Scientists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: TF-IDF."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: TF-IDF and the Computational Social Science communities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is an approach for representing text that involves the product of two components, term frequency (TF) and inverse document frequency (IDF). The term frequency is determined by dividing the number of occurrences of a specific word in a sentence by the total number of words in that sentence. The inverse document frequency is calculated by taking the logarithm of the total number of sentences in the corpus divided by the number of sentences that contain the specific word. The resulting TF-IDF score is obtained by multiplying the TF and IDF values for each word.\n",
    "\n",
    "An advantage of TF-IDF over other text representation techniques, such as the bag of words model, is its ability to give an estimation of the semantic meaning of words. While the bag of words model simply represents words as binary values (either present or absent), TF-IDF scores indicate the relative importance of words in different sentences by comparing their scores. For example, if a word is present in every sentence of a corpus, its IDF value would be log(1)=0, resulting in a TF-IDF score of 0 for that word, indicating that it does not convey any special meaning in those sentences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What you need for the next parts of the exercise:\n",
    "\n",
    "* The assignment of each author to their network community, and the degree of each author (Week 6, Exercise 4). This can be stored in a dataframe or in two dictionaries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      authorIds  Community  Degree\n",
      "0       1684687         21      10\n",
      "1    1401290379         12       5\n",
      "2       1747178         15       4\n",
      "3     145791827          1      14\n",
      "4      12619623          1       8\n",
      "..          ...        ...     ...\n",
      "427     1719389         90       0\n",
      "428     1764577          1       1\n",
      "429    38906978         91       1\n",
      "430    51440256         91       1\n",
      "431     2625318         92       0\n",
      "\n",
      "[432 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "G = nx.read_gml('css_network_update.json')\n",
    "\n",
    "communities = nx_community.louvain_communities(G)\n",
    "\n",
    "# Dictionary for community assignments and degree of each author\n",
    "data = {'authorIds': [], 'Community': [], 'Degree': []}\n",
    "\n",
    "# Loop over each author in the network\n",
    "for author in G.nodes():\n",
    "    data['authorIds'].append(author)\n",
    "    \n",
    "    for i, community in enumerate(communities):\n",
    "        if author in community:\n",
    "            data['Community'].append(i)\n",
    "            break\n",
    "    \n",
    "    data['Degree'].append(G.degree(author))\n",
    "\n",
    "# Create dataframe\n",
    "df_community = pd.DataFrame(data)\n",
    "print(df_community)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The tokenized abstract dataframe (Week 7, Exercise 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading abstract \n",
    "with open('css_abstract', \"rb\") as f:\n",
    "        css_abstract = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to tokenize\n",
    "def tokenize_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if not token in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Tokenize and save in new column 'tokens'\n",
    "css_abstract['abstract'] = css_abstract['abstract'].astype(str)\n",
    "css_abstract['tokens'] = css_abstract['abstract'].apply(tokenize_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to find out which words are important for each community, so we're going to create several *large documents, one for each community*. Each document includes all the tokens of abstracts written by members of a given community.\n",
    "\n",
    "* Consider a community c\n",
    "* Find all the abstracts of papers written by a member of community c.\n",
    "* Create a long array that stores all the abstract tokens\n",
    "* Repeat for all the communities.\n",
    "\n",
    "(To ensure your code is efficient, you shall exploit pandas builtin functions, such as groupby.apply or explode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Calculate the TF for each word. Use the method of your choice to find the top 5 terms within the top 5 communities (by number of authors).\n",
    "\n",
    "* Describe similarities and differences between the communities.\n",
    "* Why aren't the TFs not necessarily a good description of the communities?\n",
    "* Next, we calculate IDF for every word.\n",
    "* What base logarithm did you use? Is that important?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   \n",
    "We're ready to calculate TF-IDF. Do that for the top 9 communities (by number of authors). Then for each community:\n",
    "\n",
    "* List the 10 top TF words\n",
    "* List the 10 top TF-IDF words\n",
    "* List the top 3 authors (by degree)\n",
    "* Are these 10 words more descriptive of the community? If yes, what is it about IDF that makes the words more informative?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: The Wordcloud."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create word-cloud for each community. Feel free to make it as fancy or non-fancy as you like. Make sure that, together with the word cloud, you print the names of the top three authors in each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on your results. What can you conclude on the different sub-communities in Computational Social Science?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look up online the top author in each community. In light of your search, do your results make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a5c5daede66250a52d758be5d57490f14f7efe1f4cb46e79440d68554ab5513"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
